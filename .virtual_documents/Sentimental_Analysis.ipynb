


import requests
from bs4 import BeautifulSoup
import pandas as pd
import time
import random
import os

urls = [
    'https://www.amazon.in/Intel-Generation-Desktop-Processor-Warranty/dp/B09MDFH5HY/ref=sr_1_1?dib=eyJ2IjoiMSJ9.aG27SdaOhKl1C6IQGZWQVJB1NN1gT6Hj4DLkrO9cl3r_7COiobbVC2hz5c3Zyir1waCgOlpau_oGvCv8vhKq7vZFd9EWEBylEqeGYAGl9Xcwlct-AhscbQ_AGC5q_mq7rrztO6YhuhidIl9jv0GywH8L1VaBcKQUbRvrNhT9sFHeDqlu53Omw27dYOP0QhL8R_KI-QCJhoGXB8NVGNqU2-ejHm5I3URPLkIe23Jy9-w.ro4GwKXwROuzejcOgbHqRgSmdP7sk8f9woc3yOKuk9M&dib_tag=se&keywords=intel+13th+gen+processor&qid=1719047314&sr=8-1',  # 13th Gen Intel® Core desktop processors
    'https://www.amazon.in/i7-14700K-Desktop-Processor-Integrated-Graphics/dp/B0CGJ41C9W/ref=sr_1_2?crid=EFVNECX8WUIV&dib=eyJ2IjoiMSJ9.OQfGGdOmWi7_ZwojYpOcOIkblkhi8BAXnOwaKVOhqiGybM9YlNlr_hw4jxAquueKzziF2A9QpO0yPgzRFZQ5fCwkqmhy6I4zHHFxIceyAtEpWZ4cHp1Tls2GKw9hJMkS359Ug_eu3PbTsFP-CvpPlQueivxUqWT7VKU-_C1CkHtjx70_WjLOrDAtdO-nUjdCWUHDfIkpTnYbDZe6nAMy1zrcC0-bV0u-SLZvH4KA5lM.RlGeQMcvfRvYL4uz17u5aKzxDnapqfqLj693Lfw_Z8A&dib_tag=se&keywords=Intel%C2%AE+Core+desktop+processors+%2814th+gen%29&qid=1719047434&sprefix=intel+core+desktop+processors+14th+gen+%2Caps%2C224&sr=8-2',  # 14th Gen Intel® Core desktop processors
    'https://www.amazon.in/Intel-Generation-Desktop-Processor-Warranty/dp/B09MDGKQLY/ref=sr_1_1?crid=2NCOX2O7VS6N9&dib=eyJ2IjoiMSJ9.HhlMGoRTC6Ufyu1C-zRS-u8u2xtnQEXSNE7HiloC4RoFPe5tbU3z_i5V3aE6Jtlu1P8SIJg8drQZyn5dizUEMA5wbvx_mArm_FFjGw4Cj3mEPr_WHACUtOuOeAmR0H5bpjSW-v9zKKsVEzObYIQ5WyyEVODFKjtIoc3nPsrcU-94PK_qMkLMe-OIlymai6O-hZD6eYlJt7BeXblbvCzuuORt0UV5-VFgz6sUmTixPGA._9BsXXaENinzjo9gbPaY7aQsdoXmTfiE1ax3LWNl8Oc&dib_tag=se&keywords=12th+Gen+Intel%C2%AE+Core+desktop+processors&qid=1719047555&sprefix=12th+gen+intel+core+desktop+processors%2Caps%2C216&sr=8-1',  # 12th Gen Intel® Core desktop processors
    'https://www.amazon.in/i7-2670QM-SR02N-PGA988B-Mobile-Processor/dp/B008RET4D0/ref=sr_1_1?crid=2ZZAX73P6PUQ7&dib=eyJ2IjoiMSJ9.Wmbh3zHgpcXRytKgw8qJXkYR4KwM_5FIAPbIKJacrm4AcDZZrfn4AAZ7Okk7xRxGMqWPdrQsklODL1ci5aOdhQcfOtzTb8yoyPRk0UfigapO_SpVKfCUF9xoTCOUw_HkNKh55JWCUkn3vVebTS-pA1EqgY6nDVtMROhl4NOwA8r8u2f1YkTiiqjqu9oECx0p-pDfXlMjcbtwb4agDQgK6xq6dws3sESdYBEDmbtFfCI.jHyjMX8Kx9KeeqbUv6Lw4zPfhl-qdL5mzJk9v2ILRfU&dib_tag=se&keywords=intel+mobile+processor&qid=1719047597&sprefix=intel+mobile+process%2Caps%2C216&sr=8-1',  # 12th Gen Intel® Core mobile processors
    'https://www.amazon.in/Intel%C2%AE-CoreTM-i5-13400-Processor-Cache/dp/B0BN68JXR2/ref=sr_1_16?crid=2MD5O16N41NAB&dib=eyJ2IjoiMSJ9.Wmbh3zHgpcXRytKgw8qJXkYR4KwM_5FIAPbIKJacrm4AcDZZrfn4AAZ7Okk7xRxGMqWPdrQsklODL1ci5aOdhQcfOtzTb8yoyPRk0UfigapO_SpVKfCUF9xoTCOUw_HkNKh55JWCUkn3vVebTS-pA1EqgY6nDVtMROhl4NOwA8r8u2f1YkTiiqjqu9oECx0p-pDfXlMjcbtwb4agDQgK6xq6dws3sESdYBEDmbtFfCI.jHyjMX8Kx9KeeqbUv6Lw4zPfhl-qdL5mzJk9v2ILRfU&dib_tag=se&keywords=intel+mobile+processor&qid=1719047667&sprefix=intel+mobile+processor%2Caps%2C202&sr=8-16'  # 13th Gen Intel® Core mobile processors
    'https://www.flipkart.com/intel-i9-13900k-2-2-ghz-lga1700-socket-8-cores-desktop-processor/p/itme9e395e7c809c?pid=PSRGJUC2GBJNKZG5&lid=LSTPSRGJUC2GBJNKZG5AUIDRE&marketplace=FLIPKART&q=13th+Gen+Intel%C2%AE+Core+desktop+processors&store=search.flipkart.com&srno=s_1_4&otracker=search&otracker1=search&fm=Search&iid=493ac395-5b92-4cae-a22c-befb9cf52841.PSRGJUC2GBJNKZG5.SEARCH&ppt=sp&ppn=sp&ssid=je2usa3kg00000001720344818751&qH=7c00cce494b9a07e'
    'https://www.flipkart.com/intel-i7-13700k-2-5-ghz-lga1700-socket-8-cores-desktop-processor/p/itm433f25401f2a3?pid=PSRGJUC2FZYFEZUF&lid=LSTPSRGJUC2FZYFEZUF3GUE6K&marketplace=FLIPKART&q=13th+Gen+Intel%C2%AE+Core+desktop+processors&store=search.flipkart.com&srno=s_1_2&otracker=search&otracker1=search&fm=Search&iid=493ac395-5b92-4cae-a22c-befb9cf52841.PSRGJUC2FZYFEZUF.SEARCH&ppt=sp&ppn=sp&ssid=je2usa3kg00000001720344818751&qH=7c00cce494b9a07e'
    'https://www.flipkart.com/intel-i5-13600k-2-6-ghz-lga1700-socket-6-cores-desktop-processor/p/itm14dc08749bcce?pid=PSRGJUC2TYVPJ74K&lid=LSTPSRGJUC2TYVPJ74KD0SJRM&marketplace=FLIPKART&q=13th+Gen+Intel%C2%AE+Core+desktop+processors&store=search.flipkart.com&srno=s_1_1&otracker=search&otracker1=search&fm=Search&iid=493ac395-5b92-4cae-a22c-befb9cf52841.PSRGJUC2TYVPJ74K.SEARCH&ppt=sp&ppn=sp&qH=7c00cce494b9a07e'
    'https://www.flipkart.com/betaohm-3-7-ghz-lga-1151-intel-core-i3-6100-6th-generation-socket-2-cores-4-threads-3-mb-processor/p/itm1ebfe4b562894?pid=PSRGHVKHJGYWPUQT&lid=LSTPSRGHVKHJGYWPUQTQYIG8P&marketplace=FLIPKART&q=13th+Gen+Intel%C2%AE+Core+desktop+processors&store=search.flipkart.com&srno=s_1_7&otracker=search&otracker1=search&fm=Search&iid=ebee3c79-e9d9-4341-a14b-7abc49ccdaa7.PSRGHVKHJGYWPUQT.SEARCH&ppt=sp&ppn=sp&ssid=2xrl3g0ykg0000001720345001148&qH=7c00cce494b9a07e'
    'https://www.flipkart.com/wdnet-3-3-ghz-lga-1155-intel-core-i3-4th-gen-processor/p/itm0c1c2b5353820?pid=PSRGM3ED82HWCHPR&lid=LSTPSRGM3ED82HWCHPR28PPAC&marketplace=FLIPKART&q=13th+Gen+Intel%C2%AE+Core+desktop+processors&store=search.flipkart.com&srno=s_1_14&otracker=search&otracker1=search&fm=Search&iid=ebee3c79-e9d9-4341-a14b-7abc49ccdaa7.PSRGM3ED82HWCHPR.SEARCH&ppt=sp&ppn=sp&qH=7c00cce494b9a07e'
    'https://www.flipkart.com/intel-i5-3570-3-8-ghz-lga-1155-socket-4-cores-desktop-processor/p/itm97440420a734d?pid=PSRGEHETTGVYCGFM&lid=LSTPSRGEHETTGVYCGFMETDVBR&marketplace=FLIPKART&q=+Intel%C2%AE+Core+mobile+processors&store=search.flipkart.com&srno=s_1_1&otracker=search&otracker1=search&fm=Search&iid=3bba4fe9-7fcf-4b44-8de7-e22cc82d2a13.PSRGEHETTGVYCGFM.SEARCH&ppt=sp&ppn=sp&ssid=auyt637klc0000001720345106318&qH=39fe8ab88e6b633a'
    'https://www.tpstech.in/products/intel-core-i9-13900ks-unlocked-desktop-processor'
    'https://www.tpstech.in/products/intel-core-13th-gen-i7-13700k-unlocked-desktop-processor'
'https://www.tpstech.in/products/intel-core-13th-gen-i5-13400f-desktop-processor'
]

# Set up headers with a user-agent to mimic a browser
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'en-US, en;q=0.5'
}

# Function to get the HTML content of a page
def get_page_content(url, headers, retries=3):
    for i in range(retries):
        try:
            response = requests.get(url, headers=headers)
            response.raise_for_status()
            return response.content
        except requests.exceptions.HTTPError as e:
            print(f"HTTPError: {e}")
            if i < retries - 1:
                print("Retrying...")
                time.sleep(random.randint(1, 3))
            else:
                raise
        except requests.exceptions.RequestException as e:
            print(f"RequestException: {e}")
            raise

# Function to parse the reviews from the page content
def parse_reviews(page_content):
    soup = BeautifulSoup(page_content, 'html.parser')
    reviews = soup.find_all('div', {'data-hook': 'review'})
    
    review_data = []
    for review in reviews:
        try:
            title = review.find('a', {'data-hook': 'review-title'}).get_text(strip=True)
        except AttributeError:
            title = None
        try:
            rating = review.find('i', {'data-hook': 'review-star-rating'}).get_text(strip=True)
        except AttributeError:
            rating = None
        try:
            date = review.find('span', {'data-hook': 'review-date'}).get_text(strip=True)
        except AttributeError:
            date = None
        try:
            content = review.find('span', {'data-hook': 'review-body'}).get_text(strip=True)
        except AttributeError:
            content = None
        
        review_data.append({
            'title': title,
            'rating': rating,
            'date': date,
            'content': content
        })
    
    return review_data

# Function to get reviews from multiple pages
def get_reviews(url, headers, min_reviews=5000):
    reviews = []
    page = 1
    while len(reviews) < min_reviews:
        print(f"Fetching page {page} of reviews for URL: {url}")
        page_url = f"{url}&pageNumber={page}"
        page_content = get_page_content(page_url, headers)
        new_reviews = parse_reviews(page_content)
        if not new_reviews:
            break
        reviews.extend(new_reviews)
        page += 1
        time.sleep(random.randint(1, 3))  # Respectful scraping delay
    return reviews

all_reviews = []
for url in urls:
    product_reviews = get_reviews(url, headers, min_reviews=5000)
    all_reviews.extend(product_reviews)

# Create DataFrame from collected reviews
reviews_df = pd.DataFrame(all_reviews)

# Save to CSV
output_file = 'intel_amazon_reviews.csv'
if os.path.exists(output_file):
    os.remove(output_file)
reviews_df.to_csv(output_file, index=False)





import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import shap
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# Load and inspect the dataset
df = pd.read_csv('intel_products_reviews.csv')

# Extract numerical ratings
def extract_rating(rating_str):
    try:
        return float(rating_str.split()[0])
    except:
        return None

df['rating'] = df['rating'].apply(extract_rating)

# Remove numerical rating pattern from title
pattern = r'^\d+(\.\d+)? out of 5 stars'
df['title'] = df['title'].str.replace(pattern, '', regex=True).str.strip()

# Extract date and convert to datetime
def extract_date(date):
    try:
        date_parts = date.split(" on ")
        if len(date_parts) > 1:
            return date_parts[1]
        else:
            return None
    except Exception as e:
        print(f"Error processing date: {date} - {e}")
        return None

df['date'] = df['date'].apply(extract_date)
df['date'] = pd.to_datetime(df['date'], format='%d %B %Y')

# Clean text function without translation
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    text = re.sub(r'\W', ' ', text)  # Remove non-word characters
    text = text.lower()  # Convert to lowercase
    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords
    return text

# Apply cleaning to 'content' column
df['cleaned_content'] = df['content'].apply(clean_text)

# Drop original 'content' column
df.drop("content", axis=1, inplace=True)

# Displaying the first 18 rows of the DataFrame
df.head(18)







from sklearn.feature_extraction.text import TfidfVectorizer
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from sklearn.model_selection import train_test_split

# Sentiment analysis using VADER SentimentIntensityAnalyzer
def analyze_sentiment(text):
    analyzer = SentimentIntensityAnalyzer()
    sentiment_score = analyzer.polarity_scores(text)['compound']
    sentiment = 'positive' if sentiment_score > 0 else ('negative' if sentiment_score < 0 else 'neutral')
    return sentiment_score, sentiment

# Apply sentiment analysis to the DataFrame
df['sentiment_score'], df['sentiment'] = zip(*df['cleaned_content'].apply(analyze_sentiment))

# Preparing data for machine learning
def prepare_data(df, max_features=5000):
    # Filter out neutral sentiments
    df = df[df['sentiment'] != 'neutral'].copy()
    
    # Map sentiment to binary labels
    df['label'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)
    
    # Vectorize text data using TF-IDF
    vectorizer = TfidfVectorizer(max_features=max_features)
    X = vectorizer.fit_transform(df['cleaned_content']).toarray()
    y = df['label'].values
    return X, y, vectorizer

# Prepare data
X, y, vectorizer = prepare_data(df)



# Display the first 8 rows of the DataFrame with sentiment analysis results
print(df[['cleaned_content', 'sentiment_score', 'sentiment']].head(8))







df[['cleaned_content', 'sentiment_score', 'sentiment']].to_csv('sentiment_analysis_results.csv', index=False)
print("Results saved to sentiment_analysis_results.csv")






# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Display shapes of the resulting datasets
print(f"X_train shape: {X_train.shape}, X_test shape : {X_test.shape}")
print(f"y_train shape: {y_train.shape}, y_test shape : {y_test.shape}")







from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
# Logistic Regression model
logistic_model = LogisticRegression()

# Train the model
logistic_model.fit(X_train, y_train)

# Predict on the test set
y_pred = logistic_model.predict(X_test)

# Evaluate the model
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Feature importance for logistic regression
feature_importance = np.abs(logistic_model.coef_[0])
sorted_indices = np.argsort(feature_importance)[::-1]
top_features = [vectorizer.get_feature_names_out()[i] for i in sorted_indices[:25]]
print("Top Features:", top_features)






import warnings
warnings.filterwarnings('ignore')
# Deep Learning model
tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(df['cleaned_content'])
sequences = tokenizer.texts_to_sequences(df['cleaned_content'])
X = pad_sequences(sequences, maxlen=100)

X_train, X_test, y_train, y_test = train_test_split(X[:3108], y, test_size=0.2, random_state=42)

model = Sequential()
model.add(Embedding(input_dim=5000, output_dim=128, input_length=100))
model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(1, activation='sigmoid'))

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=15, batch_size=64, validation_data=(X_test, y_test))

loss, accuracy = model.evaluate(X_test, y_test)
print("LSTM Model Accuracy:", accuracy)






# SHAP interpretation for LSTM model
explainer = shap.KernelExplainer(model.predict, X_train[:100])
shap_values = explainer.shap_values(X_test[:10], nsamples=100)

shap.initjs()

# Flatten shap_values to match X_test
shap_values = np.array(shap_values).squeeze()  

# Visualize SHAP values for each sample
for i in range(len(shap_values)):
    shap.force_plot(explainer.expected_value, shap_values[i], X_test[i], feature_names=[str(j) for j in range(100)])






# Confusion Matrix
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])
disp.plot()
plt.title("Confusion Matrix for Logistic Regression")
plt.show()






df.to_csv('cleaned_data.csv', index=False)
print("Cleaned data saved to 'cleaned_data.csv'")






df.head(10)





import pickle

logistic_model_filepath = 'logistic.pkl'

with open(logistic_model_filepath, 'wb') as f:
    pickle.dump(logistic_model, f)

print(f"Logistic regression model saved to '{logistic_model_filepath}'")





from keras.models import load_model

lstm_model_filepath = 'lstm.keras'

model.save(lstm_model_filepath)

print(f"LSTM model saved to '{lstm_model_filepath}'")
